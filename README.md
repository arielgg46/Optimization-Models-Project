Este trabajo realiza un análisis exhaustivo de diferentes algoritmos de optimización aplicados a la función de Rosenbrock modificada, una función no lineal diseñada para desafiar la capacidad de los algoritmos de encontrar su mínimo global debido a la presencia de múltiples mínimos locales. Se implementaron tanto métodos clásicos como algoritmos evolutivos, poblacionales y de búsqueda estocástica. Los métodos clásicos incluyen el descenso por gradiente, Broyden–Fletcher–Goldfarb–Shanno (BFGS), el método de región de confianza y Nelder-Mead, que exploran la función utilizando gradientes pero enfrentan dificultades para evitar los mínimos locales. Por otro lado, los algoritmos evolutivos, como la evolución diferencial y el algoritmo genético, proporcionan una exploración más robusta del espacio de búsqueda mediante mecanismos de selección, mutación y cruzamiento, permitiendo escapar de estos mínimos locales. Los algoritmos basados en población, como la optimización por enjambre de partículas (PSO) y la estrategia de adaptación de la matriz de covarianza (CMA-ES), colaboran entre múltiples soluciones potenciales para mejorar la convergencia. Finalmente, los métodos de búsqueda estocástica, como el recocido simulado y el salto de cuenca, permiten explorar el espacio de soluciones aceptando temporalmente soluciones subóptimas, lo que les otorga una ventaja para encontrar el mínimo global en funciones multimodales. La comparación entre estos algoritmos se realiza considerando tanto su capacidad para hallar soluciones óptimas como su eficiencia computacional.
